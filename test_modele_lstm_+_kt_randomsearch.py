# -*- coding: utf-8 -*-
"""Test modele LSTM + kt.RandomSearch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N1lnB_XZA5L--G6kKgHE0SbK4tH2CuYz
"""

!pip install arch
!pip install tqdm
!pip install xgboost
!pip install tensorflow
!pip install keras_tuner

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from google.colab import files
import keras_tuner as kt
print("✅ keras_tuner fonctionne :", kt.__version__)

files.upload()

df = pd.read_excel('asml 2005.xlsx', header= None)
df = df.copy()
df.head()

df.columns = ['0','1','2','3','4','Date', 'Close','Volatility']
df.drop(columns=['0','1','2','3','4'], inplace=True)
df.head()

df.index = pd.to_datetime(df['Date'], format = '%Y-%m-%d')
df.drop(columns=['Date'], inplace=True)
df.head()

df.isna().sum(axis=0)

plt.plot(df['Volatility'])
plt.show()

df['Volatility'].idxmax()

features = df[['Close','Volatility']].values

scaler_close = MinMaxScaler(feature_range=(0,1))
scaler_vol = MinMaxScaler(feature_range=(0,1))

close_data = df[['Close']].values if isinstance(df['Close'], pd.Series) else df[['Close']]
vol_data = df[['Volatility']].values if isinstance(df['Volatility'], pd.Series) else df[['Volatility']]

scaled_close = scaler_close.fit_transform(close_data)
scaled_vol = scaler_vol.fit_transform(vol_data)

scaled_data = np.hstack((scaled_close, scaled_vol))

def create_sequences(data, seq_length=60):
    X,y = [],[]
    for i in range(len(data)-seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length,0])
    return np.array(X), np.array(y).reshape(-1,1)

seq_length = 60
X,y = create_sequences(scaled_data, seq_length)

split_idx = int(len(X)*0.8)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]
print(f"Forme X_train: {X_train.shape}")  # (n, 60, 2) - 60 jours, 2 features
print(f"Forme y_train: {y_train.shape}")
print(f"Forme X_test: {X_test.shape}")
print(f"Forme y_test: {y_test.shape}")

def build_model(hp):
  model = Sequential()

  for i in range(hp.Int('n_lstm_layers',1,2)):
    model.add(LSTM(
        units=hp.Int(f'units_{i}', 32, 128, step=64),
        return_sequences=(i < hp.Int('n_lstm_layers',1,2)-1),
        input_shape=(30,2) if i == 0 else None
        ))
    model.add(Dropout(hp.Float(f'dropout_{i}', 0.2, 0.4, step=0.1)))

  model.add(Dense(64))
  model.add(Dense(1))
  model.compile(
      optimizer=Adam(hp.Float('learning_rate', 1e-3, 5e-3, sampling='log')),
      loss='mean_squared_error'
  )
  return model

tuner=kt.Hyperband(build_model, objective='val_loss', max_epochs=15, directory='lstm_tuning', project_name='asml_final', factor=4)
tuner.search(X_train, y_train, epochs = 15, batch_size = 32, validation_data = (X_test, y_test), verbose=1)

tuner_rs = kt.RandomSearch(build_model, objective='val_loss', max_trials=15, directory='lstm_tuning', project_name='asml_random')
tuner_rs.search(X_train, y_train, epochs = 15, batch_size = 32, validation_data = (X_test, y_test), verbose=1)

best_model_hb = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])
best_model_hb.fit(X_train, y_train, epochs = 100, batch_size = 32, validation_data = (X_test, y_test), verbose=1)

best_model_hb.compile(
    optimizer=Adam(learning_rate=float(best_model_hb.optimizer.learning_rate.numpy())),
    loss='mean_squared_error',
    metrics=['mae']
)
loss_hb,mae_hb = best_model_hb.evaluate(X_test, y_test,verbose= 1)
print(f"Hyperband - Loss: {loss_hb}, MAE: {mae_hb}")

y_pred_hb = best_model_hb.predict(X_test)

y_test = np.array(y_test).reshape(-1)
y_pred_hb = np.array(y_pred_hb).reshape(-1)

best_model_rs = tuner_rs.hypermodel.build(tuner_rs.get_best_hyperparameters(1)[0])
best_model_rs.fit(X_train, y_train, epochs = 100, batch_size = 32, validation_data = (X_test, y_test), verbose=1)

best_model_rs.compile(
    optimizer=Adam(learning_rate=float(best_model_rs.optimizer.learning_rate.numpy())),
    loss='mean_squared_error',
    metrics=['mae']
)
loss,mae = best_model_rs.evaluate(X_test, y_test,verbose= 1)
print(f"Random search -- Loss: {loss}, MAE: {mae}")

y_pred_rs = best_model_rs.predict(X_test).reshape(-1)

plt.figure(figsize=(12,6))
plt.plot(y_test, label='Valeurs réelles', linewidth=2, c='b')
plt.plot(y_pred_hb, label='Prédictions du modèle Hyperband', linestyle='--', c='r')
plt.plot(y_pred_rs, label='Prédictions du modèle Random search', linestyle='-', c='orange')
plt.title("Comparaison entre les valeurs réelles et les prédictions des 2 modèles")
plt.xlabel("Échantillons")
plt.ylabel("Valeurs")
plt.legend()
plt.show()