# -*- coding: utf-8 -*-
"""VaR test + monte carlo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qL-WZSOMnjDhqBLqrerZMv9qzNk7wvbm
"""

# ============================================================
# Monte Carlo VaR/ES multi-facteurs à partir de Yahoo Finance
# - Télécharge données (actions + taux 10Y US)
# - Calcule rendements toi-même
# - EWMA vol conditionnelle + copule gaussienne
# - Full-reval (actions: expos; taux: duration/DV01)
# - VaR/ES + Component ES
# ============================================================

import numpy as np
import pandas as pd
from scipy.linalg import cholesky
from pathlib import Path

# --- Installations utiles si nécessaire ---
try:
    import yfinance as yf
except ImportError:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "yfinance", "-q"])
    import yfinance as yf

# -----------------------------
# 0) Paramètres généraux
# -----------------------------
ALPHA       = 0.99          # niveau de confiance VaR/ES
N_SIMS      = 100_000       # nb scénarios Monte Carlo
EWMA_LAMBDA = 0.94          # lambda RiskMetrics (quotidien)
START       = "2018-01-01"  # fenêtre d'historique
END         = None          # None = aujourd'hui
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# -----------------------------
# 1) Portefeuille et mapping Yahoo
# -----------------------------
# FACTORS: noms "métiers" que tu suis pour l'attribution
# MAP_YF : tickers Yahoo correspondants
#   - S&P 500 cash: ^GSPC
#   - EURO STOXX 50: ^STOXX50E
#   - CAC 40: ^FCHI
#   - US 10Y yield: ^TNX  (ATTENTION: ^TNX est "yield * 10"; 45.6 => 4.56%)
FACTORS = ["SPX", "ESTX50", "CAC40", "US10Y"]
MAP_YF  = {"SPX": "^GSPC", "ESTX50": "^STOXX50E", "CAC40": "^FCHI", "US10Y": "^TNX"}

# Portefeuille d'exemple (adapte valeurs)
portfolio = [
    {"name": "SPX",    "type": "equity", "exposure": 4_000_000.0},
    {"name": "ESTX50", "type": "equity", "exposure": 3_000_000.0},
    {"name": "CAC40",  "type": "equity", "exposure": 1_000_000.0},
    {"name": "US10Y",  "type": "rate",   "md": 7.0, "value": 2_000_000.0},  # duration * notionnel
    # Variante DV01 si tu préfères:
    # {"name": "US10Y",  "type": "rate",   "dv01": 850.0},  # en $/bp
]

# -----------------------------
# 2) Téléchargement Yahoo + calcul des rendements/jours
# -----------------------------
def fetch_from_yahoo(map_yf, start=START, end=END):
    tickers = list(map_yf.values())
    data = yf.download(tickers, start=start, end=end, auto_adjust=True, progress=False)
    # On prend la colonne 'Close' si multi-index, sinon DataFrame direct
    if isinstance(data.columns, pd.MultiIndex):
        px = data["Close"].copy()
    else:
        px = data.copy()
    px.columns = tickers
    return px

def compute_factor_returns(px, map_yf):
    """
    - Actions/indices: returns = pct_change
    - US10Y (^TNX): on convertit en 'yield décimal' puis Δr = diff quotidienne (pas pct_change)
    """
    df = pd.DataFrame(index=px.index)
    # Actions/indices
    for f, yf_ticker in map_yf.items():
        if f != "US10Y":
            df[f] = px[yf_ticker].pct_change()
    # Taux US10Y
    # ^TNX: ex 45.67 => 4.567% => 0.04567 en décimal
    tnx = px[map_yf["US10Y"]].copy()
    y_dec = (tnx / 10_0_0)  #  ^TNX / 1000 => décimal (ex 45.6 -> 0.0456)
    # NB: 10_0_0 = 1000 (underscore pour lisibilité)
    df["US10Y"] = y_dec.diff()  # Δr en décimal (ex 3 bps -> 0.0003)
    # Alignement & drop NA initial
    df = df.dropna(how="any")
    return df

px = fetch_from_yahoo(MAP_YF, START, END)
returns = compute_factor_returns(px, MAP_YF)
returns = returns[FACTORS]  # ordre cohérent

# -----------------------------
# 3) Volatilités conditionnelles EWMA + corrélation robuste
# -----------------------------
def ewma_vol(series, lam=EWMA_LAMBDA):
    var = series.var()  # init
    for r in series.values:
        var = lam * var + (1 - lam) * (r ** 2)
    return np.sqrt(var)

sigma_cond = np.array([ewma_vol(returns[c]) for c in FACTORS])  # vols du jour

# Corrélation robuste via Kendall->Pearson : rho = sin(pi/2 * tau)
tau = returns.corr(method="kendall").values
rho = np.sin(np.pi * tau / 2.0)

# Stabilisation PSD
def make_psd(mat, eps=1e-8):
    try:
        _ = cholesky(mat, lower=True)
        return mat
    except np.linalg.LinAlgError:
        d = mat.shape[0]
        return mat + eps * np.eye(d)

rho = make_psd(rho)

# -----------------------------
# 4) Simulateur copule gaussienne
# -----------------------------
from scipy.stats import norm

def simulate_gaussian_copula(n_sims, corr, sigmas):
    d = len(sigmas)
    L = cholesky(corr, lower=True)
    Z = np.random.randn(n_sims, d) @ L.T            # corrélés ~ N(0, corr)
    shocks = Z * sigmas.reshape(1, -1)              # applique vols conditionnelles
    return shocks

# -----------------------------
# 5) PnL par scénario
# -----------------------------
def pnl_by_scenario(shocks, portfolio, factors):
    """
    shocks[:, j]:
      - equity: rendement
      - rate:   Δr (décimal)
    """
    n_sims, d = shocks.shape
    pnl_mat = np.zeros((n_sims, d))
    for j, p in enumerate(portfolio):
        s = shocks[:, j]
        if p["type"] == "equity":
            pnl_mat[:, j] = p["exposure"] * s
        elif p["type"] == "rate":
            if "dv01" in p:
                pnl_mat[:, j] = p["dv01"] * (s * 10_000.0)  # Δbp = Δr * 10_000
            elif "md" in p and "value" in p:
                pnl_mat[:, j] = -p["md"] * s * p["value"]   # -MD * Δr * notionnel
            else:
                raise ValueError("Pour 'rate', fournir 'dv01' OU ('md' et 'value').")
        else:
            raise ValueError(f"Type inconnu: {p['type']}")
    return pnl_mat.sum(axis=1), pd.DataFrame(pnl_mat, columns=factors)

# -----------------------------
# 6) VaR/ES + Component ES
# -----------------------------
def var_es(pnl, alpha=ALPHA, weights=None):
    pnl = np.asarray(pnl)
    if weights is None:
        weights = np.ones_like(pnl) / len(pnl)
    order = np.argsort(pnl)               # pertes (négatives) en premier
    pnl_sorted = pnl[order]
    w_sorted   = weights[order]
    cw = np.cumsum(w_sorted)
    # index du quantile (1 - alpha) en queue à gauche
    var_index = np.searchsorted(cw, (1 - alpha))
    var_value = -pnl_sorted[var_index]    # >0
    tail_mask = cw <= (1 - alpha) + 1e-12
    if not tail_mask.any():
        es_value = var_value
    else:
        es_value = -np.sum(pnl_sorted[tail_mask] * w_sorted[tail_mask]) / np.sum(w_sorted[tail_mask])
    return var_value, es_value, order, tail_mask, (pnl_sorted, w_sorted)

def component_es(pnl_breakdown, order, tail_mask, w_sorted):
    B = pnl_breakdown.values[order]
    tail_B = B[tail_mask]
    tail_w = w_sorted[tail_mask].reshape(-1, 1)
    comp = - (tail_B * tail_w).sum(axis=0) / tail_w.sum()
    return pd.Series(comp, index=pnl_breakdown.columns)

# -----------------------------
# 7) Simulation & résultats
# -----------------------------
shocks = simulate_gaussian_copula(N_SIMS, rho, sigma_cond)
total_pnl, pnl_factors = pnl_by_scenario(shocks, portfolio, FACTORS)
VaR, ES, order, tail_mask, (_p_sorted, w_sorted) = var_es(total_pnl, alpha=ALPHA, weights=None)
compES = component_es(pnl_factors, order, tail_mask, w_sorted)

print(f"--- Monte Carlo (Yahoo Finance) ---")
print(f"Historique: {returns.index[0].date()} → {returns.index[-1].date()}  ({len(returns):,} jours)")
print(f"alpha = {ALPHA:.2%}, simulations = {N_SIMS:,}")
print(f"VaR : {VaR:,.0f}")
print(f"ES  : {ES:,.0f}")
print("\nComponent ES (contributions qui ~somment vers ES) :")
print(compES.sort_values(ascending=False).round(0))
print(f"\nSomme components ≈ {compES.sum():,.0f}  (vs ES {ES:,.0f})")

# ===========================================
# Monte Carlo 1-jour (simple)
# - Yahoo Finance
# - Rendements calculés par nous
# - Vol sample + corr sample
# - Copule gaussienne (Cholesky)
# - PnL simple: actions (exposure*return)
#               taux 10Y US: PnL ≈ -MD * Δr * notionnel
# - VaR/ES (alpha=99%)
# ===========================================

import numpy as np
import pandas as pd

# --- yfinance install si besoin ---
try:
    import yfinance as yf
except ImportError:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "yfinance", "-q"])
    import yfinance as yf

# ======================
# 0) Paramètres simples
# ======================
ALPHA   = 0.99      # niveau de confiance
N_SIMS  = 50_000    # nb scénarios MC (rapide)
START   = "2019-01-01"
END     = None      # None = aujourd'hui
SEED    = 42
np.random.seed(SEED)

# Portefeuille (exemple très simple)
#  - 3 indices actions (expositions en €)
#  - 1 facteur taux (US10Y) via duration modifiée
PORTFOLIO = [
    {"name": "SPX",   "type": "equity", "exposure": 4_000_000.0},   # S&P 500
    {"name": "ESTX50","type": "equity", "exposure": 3_000_000.0},   # EURO STOXX 50
    {"name": "CAC40", "type": "equity", "exposure": 1_000_000.0},   # CAC 40
    {"name": "US10Y", "type": "rate",   "md": 7.0, "value": 2_000_000.0},  # duration * notionnel
]

# Mapping vers Yahoo
FACTORS = ["SPX","ESTX50","CAC40","US10Y"]
MAP_YF  = {"SPX":"^GSPC","ESTX50":"^STOXX50E","CAC40":"^FCHI","US10Y":"^TNX"}  # ^TNX = yield*10

# ======================
# 1) Données & rendements
# ======================
def fetch_prices(map_yf, start=START, end=END):
    tickers = list(map_yf.values())
    data = yf.download(tickers, start=start, end=end, auto_adjust=True, progress=False)
    px = data["Close"] if isinstance(data.columns, pd.MultiIndex) else data
    px.columns = tickers
    return px

def compute_returns(px, map_yf):
    df = pd.DataFrame(index=px.index)
    # actions: pourcentages
    for f, t in map_yf.items():
        if f != "US10Y":
            df[f] = px[t].pct_change()
    # taux: ^TNX => ex 45.6 -> 0.0456; Δr = diff (en décimal)
    y_dec = px[map_yf["US10Y"]] / 1000.0
    df["US10Y"] = y_dec.diff()
    return df.dropna(how="any")

px = fetch_prices(MAP_YF)
rets = compute_returns(px, MAP_YF)[FACTORS]

# ======================
# 2) Vol & corr (sample)
# ======================
sigmas = rets.std().values                 # écarts-types journaliers
rho    = rets.corr().values                # corrélations
Sigma  = np.outer(sigmas, sigmas) * rho    # matrice de covariance

# ======================
# 3) Monte Carlo (gaussien)
# ======================
# Chocs ~ N(0, Sigma) via Cholesky
L = np.linalg.cholesky(Sigma)
Z = np.random.randn(N_SIMS, len(FACTORS))
shocks = Z @ L.T   # shape (N_SIMS, d)

# ======================
# 4) PnL par scénario (full reval simple)
# ======================
pnl_mat = np.zeros_like(shocks)
for j, p in enumerate(PORTFOLIO):
    s = shocks[:, j]
    if p["type"] == "equity":
        pnl_mat[:, j] = p["exposure"] * s
    elif p["type"] == "rate":
        pnl_mat[:, j] = -p["md"] * s * p["value"]    # ΔP ≈ -MD * Δr * notionnel

total_pnl = pnl_mat.sum(axis=1)

# ======================
# 5) VaR / ES (alpha)
# ======================
order = np.argsort(total_pnl)        # pertes (négatives) d'abord
sorted_pnl = total_pnl[order]
n = len(sorted_pnl)
k = int(np.ceil((1-ALPHA)*n))        # taille de la queue (≈ 1%)
VaR = -sorted_pnl[k-1]               # >0  (opposé du quantile)
ES  = -sorted_pnl[:k].mean()         # moyenne des pertes de la queue

print(f"Fenêtre: {rets.index[0].date()} → {rets.index[-1].date()}  (jours: {len(rets):,})")
print(f"Sims: {N_SIMS:,} | alpha: {ALPHA:.2%}")
print(f"VaR: {VaR:,.0f}")
print(f"ES : {ES:,.0f}")

