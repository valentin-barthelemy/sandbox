# -*- coding: utf-8 -*-
"""Test modele XGBOOST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UpEH4ajYfFdATzWb0c8vjQUdA8tqhHF2
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
from google.colab import files

files.upload()

df = pd.read_excel('asml 2005.xlsx', header = None)
df = df.copy()
df.head()

df.columns = ['0','1','2','3','4','Date','Close','Volatility']
df.drop (columns=['0','1','2','3','4'], inplace=True)
df.head()

df.index = pd.to_datetime(df['Date'], format = '%Y-%m-%d')
df.drop(columns=['Date'], inplace=True)
df.head()

df.isna().sum(axis=0)

plt.plot(df['Close'])
plt.show()

plt.plot(df['Volatility'])
plt.show()

lag_features = 5

for i in range (1, lag_features + 1):
  df['Close_Lag_' + str(i)] = df['Close'].shift(i)

df['Volatility_Lag_1'] = df['Volatility'].shift(1)

df.dropna(inplace=True)
df.head()

df.info()

feature_cols = [f'Close_Lag_{i}' for i in range (1, lag_features + 1)]+['Volatility_Lag_1']
X = df[feature_cols].values
y = df['Close'].values

print(f'X shape: {X.shape}, y shape: {y.shape}')
print (f'Features used : {feature_cols}')

split_idx = int(len(df) * 0.8)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

xgb = XGBRegressor(random_state=42, n_jobs=-1)

param_grid = {
    'n_estimators': [100, 200, 500],
    'learning_rate': [0.001, 0.01, 0.05, 0.1],
    'max_depth': [5, 7],
    'subsample': [0.6 ,0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'min_child_weight': [1,3]
}

tscv = TimeSeriesSplit(n_splits=5)

grid_search = GridSearchCV(estimator = xgb,
                           param_grid=param_grid,
                           scoring='neg_mean_squared_error',
                           cv=tscv,
                           verbose=1,
                           n_jobs=-1)

grid_search.fit(X_train, y_train)

print(f'Best parameters: {grid_search.best_params_}')
print(f'Best score: {grid_search.best_score_}')

train_score = grid_search.score(X_train, y_train)
test_score = grid_search.score(X_test, y_test)

print(f'Train score: {train_score}')
print(f'Test score: {test_score}')

y_pred = grid_search.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'MSE: {mse}')

importances = grid_search.best_estimator_.feature_importances_
for feat, imp in zip(feature_cols, importances):
  print(f'{feat}: {imp}')

plt.figure(figsize=(12,6))
plt.plot(y_test, label='Valeurs réelles', linewidth=2)
plt.plot(y_pred, label='Prédictions', linestyle='-')
plt.title("Comparaison entre valeurs réelles et prédictions XGBOOST")
plt.xlabel("Échantillons")
plt.ylabel("Prix")
plt.legend()
plt.savefig("comparaison_plot.png")
plt.show()

files.download("comparaison_plot.png")